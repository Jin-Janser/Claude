# 필수 라이브러리 임포트
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from scipy.spatial.distance import pdist, squareform
import warnings
warnings.filterwarnings('ignore')

# 1. 데이터 로딩 및 기본 탐색
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target_names[iris.target]

print("=== 데이터 기본 정보 ===")
print(f"데이터 형태: {df.shape}")
print(f"결측치 확인: {df.isnull().sum().sum()}")
print("\n기술통계량 (원본 데이터):")
print(df.describe())

# 2. 이상치 탐지 및 처리 (IQR × 1.5)
numeric_columns = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
Q1 = df[numeric_columns].quantile(0.25)
Q3 = df[numeric_columns].quantile(0.75)
IQR = Q3 - Q1

# 이상치 범위 계산
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# 이상치 식별
outliers = ((df[numeric_columns] < lower_bound) | (df[numeric_columns] > upper_bound)).any(axis=1)
print(f"\n이상치 개수: {outliers.sum()}개")

# 이상치 제거
df_clean = df[~outliers].reset_index(drop=True)
print(f"이상치 제거 후 데이터 형태: {df_clean.shape}")

# 3. 표준화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_clean[numeric_columns])
X_scaled_df = pd.DataFrame(X_scaled, columns=numeric_columns)

print("\n기술통계량 (표준화 후):")
print(X_scaled_df.describe())

# PCA 수행
pca = PCA()
pca_result = pca.fit_transform(X_scaled)

# 고유값과 설명된 분산 비율
eigenvalues = pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# PCA 결과표 생성
pca_results = pd.DataFrame({
    'PC': [f'PC{i+1}' for i in range(len(eigenvalues))],
    '고유값': eigenvalues,
    '설명된_분산_비율': explained_variance_ratio,
    '누적_분산_비율': cumulative_variance
})

print("=== PCA 결과표 ===")
print(pca_results.round(4))

# 변수 기여도 (Loadings)
loadings = pd.DataFrame(
    pca.components_.T,
    columns=[f'PC{i+1}' for i in range(len(eigenvalues))],
    index=numeric_columns
)

print("\n=== 변수 기여도 (PC1, PC2) ===")
print(loadings[['PC1', 'PC2']].round(4))

# 스크리 플롯
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(eigenvalues)+1), eigenvalues, 'bo-', linewidth=2, markersize=8)
plt.title('Scree Plot - Eigenvalues by Principal Component', fontsize=14)
plt.xlabel('Principal Component', fontsize=12)
plt.ylabel('Eigenvalue', fontsize=12)
plt.grid(True, alpha=0.3)
plt.xticks(range(1, len(eigenvalues)+1))
plt.show()

# Biplot 생성
plt.figure(figsize=(12, 8))
pc1_scores = pca_result[:, 0]
pc2_scores = pca_result[:, 1]

# 품종별 색상 구분
colors = ['red', 'blue', 'green']
species_names = df_clean['species'].unique()

for i, species in enumerate(species_names):
    mask = df_clean['species'] == species
    plt.scatter(pc1_scores[mask], pc2_scores[mask], 
               c=colors[i], label=species, alpha=0.7, s=50)

# Loading vectors 추가
for i, var in enumerate(numeric_columns):
    plt.arrow(0, 0, loadings.iloc[i, 0]*3, loadings.iloc[i, 1]*3,
              head_width=0.1, head_length=0.1, fc='black', ec='black')
    plt.text(loadings.iloc[i, 0]*3.2, loadings.iloc[i, 1]*3.2, 
             var.replace(' (cm)', ''), fontsize=10, ha='center')

plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}% variance)', fontsize=12)
plt.ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}% variance)', fontsize=12)
plt.title('PCA Biplot - Iris Dataset', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Dunn Index 계산 함수
def dunn_index(X, labels):
    """Dunn Index 계산"""
    distances = squareform(pdist(X))
    unique_labels = np.unique(labels)
    
    # Intra-cluster distances (최대값)
    intra_dists = []
    for label in unique_labels:
        cluster_points = np.where(labels == label)[0]
        if len(cluster_points) > 1:
            cluster_distances = distances[np.ix_(cluster_points, cluster_points)]
            intra_dists.append(np.max(cluster_distances))
    
    # Inter-cluster distances (최소값)
    inter_dists = []
    for i, label1 in enumerate(unique_labels):
        for label2 in unique_labels[i+1:]:
            cluster1_points = np.where(labels == label1)[0]
            cluster2_points = np.where(labels == label2)[0]
            inter_cluster_distances = distances[np.ix_(cluster1_points, cluster2_points)]
            inter_dists.append(np.min(inter_cluster_distances))
    
    return np.min(inter_dists) / np.max(intra_dists)

# K-means 군집 분석 (k=3)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(X_scaled)

# 계층적 군집 분석
hierarchical_linkage = linkage(X_scaled, method='ward')
hierarchical_labels = fcluster(hierarchical_linkage, t=3, criterion='maxclust') - 1

# 실루엣 계수 계산
silhouette_kmeans = silhouette_score(X_scaled, kmeans_labels)
silhouette_hierarchical = silhouette_score(X_scaled, hierarchical_labels)

# Dunn 지수 계산
dunn_kmeans = dunn_index(X_scaled, kmeans_labels)
dunn_hierarchical = dunn_index(X_scaled, hierarchical_labels)

# 군집 분석 결과표
cluster_results = pd.DataFrame({
    'Method': ['K-means', 'Hierarchical'],
    'Silhouette_Score': [silhouette_kmeans, silhouette_hierarchical],
    'Dunn_Index': [dunn_kmeans, dunn_hierarchical]
})

print("=== 군집 분석 결과 비교 ===")
print(cluster_results.round(4))

# PCA 공간에 군집 결과 오버레이
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# 원본 품종별 분포
for i, species in enumerate(species_names):
    mask = df_clean['species'] == species
    ax1.scatter(pc1_scores[mask], pc2_scores[mask], 
               c=colors[i], label=species, alpha=0.7, s=50)
ax1.set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
ax1.set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
ax1.set_title('True Species Labels')
ax1.legend()
ax1.grid(True, alpha=0.3)

# K-means 결과
scatter = ax2.scatter(pc1_scores, pc2_scores, c=kmeans_labels, 
                     cmap='viridis', alpha=0.7, s=50)
ax2.set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
ax2.set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
ax2.set_title(f'K-means Clustering (Silhouette: {silhouette_kmeans:.3f})')
ax2.grid(True, alpha=0.3)
plt.colorbar(scatter, ax=ax2)

# 계층적 군집 결과
scatter = ax3.scatter(pc1_scores, pc2_scores, c=hierarchical_labels, 
                     cmap='plasma', alpha=0.7, s=50)
ax3.set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
ax3.set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
ax3.set_title(f'Hierarchical Clustering (Silhouette: {silhouette_hierarchical:.3f})')
ax3.grid(True, alpha=0.3)
plt.colorbar(scatter, ax=ax3)

# 덴드로그램
dendrogram(hierarchical_linkage, ax=ax4, truncate_mode='lastp', p=10)
ax4.set_title('Hierarchical Clustering Dendrogram')
ax4.set_xlabel('Sample Index')
ax4.set_ylabel('Distance')

plt.tight_layout()
plt.show()

# 실루엣 플롯
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# K-means 실루엣 플롯
silhouette_samples_kmeans = silhouette_samples(X_scaled, kmeans_labels)
y_lower = 10
for i in range(3):
    cluster_silhouette_values = silhouette_samples_kmeans[kmeans_labels == i]
    cluster_silhouette_values.sort()
    
    size_cluster_i = cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    
    color = plt.cm.nipy_spectral(float(i) / 3)
    ax1.fill_betweenx(np.arange(y_lower, y_upper),
                     0, cluster_silhouette_values,
                     facecolor=color, edgecolor=color, alpha=0.7)
    
    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

ax1.axvline(x=silhouette_kmeans, color="red", linestyle="--")
ax1.set_xlabel('Silhouette Coefficient Values')
ax1.set_ylabel('Cluster Label')
ax1.set_title('K-means Silhouette Plot')

# 계층적 군집 실루엣 플롯
silhouette_samples_hier = silhouette_samples(X_scaled, hierarchical_labels)
y_lower = 10
for i in range(3):
    cluster_silhouette_values = silhouette_samples_hier[hierarchical_labels == i]
    cluster_silhouette_values.sort()
    
    size_cluster_i = cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    
    color = plt.cm.nipy_spectral(float(i) / 3)
    ax2.fill_betweenx(np.arange(y_lower, y_upper),
                     0, cluster_silhouette_values,
                     facecolor=color, edgecolor=color, alpha=0.7)
    
    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

ax2.axvline(x=silhouette_hierarchical, color="red", linestyle="--")
ax2.set_xlabel('Silhouette Coefficient Values')
ax2.set_ylabel('Cluster Label')
ax2.set_title('Hierarchical Silhouette Plot')

plt.tight_layout()
plt.show()

# 스케일링 전후 변수 분포 비교
fig, axes = plt.subplots(2, 4, figsize=(16, 8))

for i, col in enumerate(numeric_columns):
    # 원본 분포
    axes[0, i].hist(df_clean[col], bins=20, alpha=0.7, color='skyblue')
    axes[0, i].set_title(f'Original: {col.replace(" (cm)", "")}')
    axes[0, i].set_ylabel('Frequency')
    
    # 표준화 후 분포
    axes[1, i].hist(X_scaled[:, i], bins=20, alpha=0.7, color='lightcoral')
    axes[1, i].set_title(f'Standardized: {col.replace(" (cm)", "")}')
    axes[1, i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()
