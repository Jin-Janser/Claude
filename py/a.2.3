import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# 1. 데이터 로딩 및 전처리
def load_and_preprocess_data():
    # 실제 데이터 로딩 (경로 수정 필요)
    # red_wine = pd.read_csv('winequality-red.csv', sep=';')
    # white_wine = pd.read_csv('winequality-white.csv', sep=';')
    
    # 샘플 데이터 생성 (실제 데이터 부재 시)
    np.random.seed(42)
    n_samples = 6497
    
    data = pd.DataFrame({
        'fixed_acidity': np.random.normal(8.32, 1.74, n_samples),
        'volatile_acidity': np.random.gamma(2, 0.2, n_samples),
        'citric_acid': np.random.beta(2, 8, n_samples),
        'residual_sugar': np.random.exponential(2.5, n_samples),
        'chlorides': np.random.gamma(3, 0.03, n_samples),
        'free_sulfur_dioxide': np.random.gamma(2, 15, n_samples),
        'total_sulfur_dioxide': np.random.normal(46, 32, n_samples),
        'density': np.random.normal(0.9967, 0.0019, n_samples),
        'pH': np.random.normal(3.31, 0.15, n_samples),
        'sulphates': np.random.gamma(4, 0.15, n_samples),
        'alcohol': np.random.normal(10.4, 1.07, n_samples),
        'quality': np.random.choice(range(3, 10), n_samples, 
                                  p=[0.01, 0.05, 0.42, 0.40, 0.11, 0.01, 0.00])
    })
    
    # 특성 변수 표준화
    feature_cols = [col for col in data.columns if col != 'quality']
    scaler = StandardScaler()
    data[feature_cols] = scaler.fit_transform(data[feature_cols])
    
    return data, feature_cols

# 2. 베이지안 회귀 모델 구축
def build_bayesian_model(data, feature_cols, prior_type='weakly_informative'):
    X = data[feature_cols].values
    y = data['quality'].values
    
    with pm.Model() as model:
        # 사전 분포 설정
        if prior_type == 'non_informative':
            # 비정보적 사전분포 (평평한 분포)
            beta = pm.Normal('beta', mu=0, sigma=100, shape=len(feature_cols))
            alpha = pm.Normal('alpha', mu=0, sigma=100)
        else:
            # 약하게 정보적 사전분포
            beta = pm.Normal('beta', mu=0, sigma=1, shape=len(feature_cols))
            alpha = pm.Normal('alpha', mu=y.mean(), sigma=1)
        
        # 오차항의 사전분포
        sigma = pm.HalfNormal('sigma', sigma=1)
        
        # 선형 회귀 모델
        mu = alpha + pm.math.dot(X, beta)
        
        # 가능도 함수
        likelihood = pm.Normal('y', mu=mu, sigma=sigma, observed=y)
        
        # MCMC 샘플링
        trace = pm.sample(2000, tune=1000, cores=2, 
                         target_accept=0.95, return_inferencedata=True)
    
    return model, trace

# 3. 모델 비교 함수
def compare_models(data, feature_cols):
    X = data[feature_cols].values
    y = data['quality'].values
    
    models = {}
    traces = {}
    
    # 기본 모델
    with pm.Model() as basic_model:
        beta = pm.Normal('beta', mu=0, sigma=1, shape=len(feature_cols))
        alpha = pm.Normal('alpha', mu=y.mean(), sigma=1)
        sigma = pm.HalfNormal('sigma', sigma=1)
        
        mu = alpha + pm.math.dot(X, beta)
        likelihood = pm.Normal('y', mu=mu, sigma=sigma, observed=y)
        
        trace_basic = pm.sample(1000, tune=500, cores=2, return_inferencedata=True)
    
    models['basic'] = basic_model
    traces['basic'] = trace_basic
    
    # 상호작용 모델 (주요 변수들만)
    with pm.Model() as interaction_model:
        beta = pm.Normal('beta', mu=0, sigma=1, shape=len(feature_cols))
        # 알코올과 휘발성산도의 상호작용
        interaction = pm.Normal('interaction', mu=0, sigma=1)
        alpha = pm.Normal('alpha', mu=y.mean(), sigma=1)
        sigma = pm.HalfNormal('sigma', sigma=1)
        
        mu = (alpha + pm.math.dot(X, beta) + 
              interaction * X[:, 0] * X[:, 1])  # 첫 번째와 두 번째 변수의 상호작용
        likelihood = pm.Normal('y', mu=mu, sigma=sigma, observed=y)
        
        trace_interaction = pm.sample(1000, tune=500, cores=2, return_inferencedata=True)
    
    models['interaction'] = interaction_model
    traces['interaction'] = trace_interaction
    
    # 모델 비교
    model_comparison = az.compare(traces, ic='waic')
    
    return models, traces, model_comparison

# 4. 시각화 함수
def create_visualizations(trace, feature_cols):
    # 사후 분포 시각화
    az.plot_posterior(trace, var_names=['beta'], 
                     coords={'beta_dim_0': range(len(feature_cols))},
                     figsize=(12, 8))
    plt.gca().set_xticklabels(feature_cols, rotation=45)
    plt.title('Posterior Distributions of Regression Coefficients')
    plt.tight_layout()
    plt.show()
    
    # Trace plot
    az.plot_trace(trace, var_names=['beta'], compact=True, figsize=(12, 8))
    plt.suptitle('MCMC Trace Plots')
    plt.tight_layout()
    plt.show()

# 메인 실행
def main():
    # 데이터 로딩
    data, feature_cols = load_and_preprocess_data()
    
    print("데이터 기본 정보:")
    print(f"샘플 수: {len(data)}")
    print(f"변수 수: {len(feature_cols)}")
    print(f"품질 분포:\n{data['quality'].value_counts().sort_index()}")
    
    # 약하게 정보적 사전분포 모델
    model_weak, trace_weak = build_bayesian_model(data, feature_cols, 'weakly_informative')
    
    # 비정보적 사전분포 모델  
    model_flat, trace_flat = build_bayesian_model(data, feature_cols, 'non_informative')
    
    # 수렴 진단
    print("\n=== 수렴 진단 (Weakly Informative) ===")
    print(az.summary(trace_weak, var_names=['beta'], round_to=3))
    
    # 사전분포 민감도 분석
    print("\n=== 사전분포 민감도 분석 ===")
    summary_weak = az.summary(trace_weak, var_names=['beta'])
    summary_flat = az.summary(trace_flat, var_names=['beta'])
    
    comparison_df = pd.DataFrame({
        'Variable': feature_cols,
        'Weak_Mean': summary_weak['mean'].values,
        'Weak_CI_Lower': summary_weak['hdi_2.5%'].values,
        'Weak_CI_Upper': summary_weak['hdi_97.5%'].values,
        'Flat_Mean': summary_flat['mean'].values,
        'Flat_CI_Lower': summary_flat['hdi_2.5%'].values,
        'Flat_CI_Upper': summary_flat['hdi_97.5%'].values
    })
    
    comparison_df['Mean_Diff'] = abs(comparison_df['Weak_Mean'] - comparison_df['Flat_Mean'])
    print(comparison_df.round(3))
    
    # 모델 비교
    print("\n=== 모델 비교 ===")
    models, traces, model_comp = compare_models(data, feature_cols)
    print(model_comp)
    
    # 시각화
    create_visualizations(trace_weak, feature_cols)
    
    return model_weak, trace_weak, comparison_df, model_comp

if __name__ == "__main__":
    model, trace, comparison, model_comparison = main()
